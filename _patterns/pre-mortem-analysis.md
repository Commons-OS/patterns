---
# ═══════════════════════════════════════════════════════════════════
# GROUP 1: CORE IDENTITY
# ═══════════════════════════════════════════════════════════════════
id: pat_01khvcxcahfcq8epkdrbdfzf0y
slug: pre-mortem-analysis
title: "Pre-Mortem Analysis"
aliases: []
summary: >-
  Before committing to a major life decision, imagine it has already
  failed and work backward to identify likely causes and preventable
  risks.

# ═══════════════════════════════════════════════════════════════════
# GROUP 2: CONTEXTUAL TRANSLATION (The Navigator Engine)
# ═══════════════════════════════════════════════════════════════════
context_labels:
  corporate: "Project Risk Assessment"
  government: "Policy Pre-Mortem"
  activist: "Campaign Risk Analysis"
  tech: "Pre-Mortem AI Facilitator"

# ═══════════════════════════════════════════════════════════════════
# GROUP 3: ONTOLOGY & SEARCH OPTIMIZATION (The RAG Fuel)
# ═══════════════════════════════════════════════════════════════════
ontology:
  domain: time-productivity
  cross_domains: []
  commons_domain:
    - life
  search_hints:
    primary_tension: "Pre vs. Analysis"
    vector_keywords: ["pre mortem", "analysis", "before", "committing", "major"]
  commons_assessment:
    stakeholder_architecture: 3.0
    value_creation: 3.5
    resilience: 3.0
    ownership: 4.0
    autonomy: 3.0
    composability: 3.0
    fractal_value: 4.0
    vitality: 3.5
    vitality_reasoning: >-
      This pattern sustains vitality by maintaining and renewing the
      system's existing health. 'Pre-Mortem Analysis' contributes to
      ongoing functioning without necessarily generating new adaptive
      capacity. Watch for signs of rigidity if implementation becomes
      routinised.
    overall_score: 3.4

# ═══════════════════════════════════════════════════════════════════
# GROUP 4: LIFECYCLE & CONFIDENCE
# ═══════════════════════════════════════════════════════════════════
lifecycle:
  usage_stage: application
  adoption_stage: growth
  status: draft
  version: 0.1
  confidence: 1

# ═══════════════════════════════════════════════════════════════════
# GROUP 5: HARD RELATIONSHIPS (Human-Curated Graph)
# ═══════════════════════════════════════════════════════════════════
relationships:
  generalizes_from:
    - slug: adaptive-strategy-under-uncertainty
      weight: 0.85
    - slug: scenario-planning
      weight: 0.82
  specializes_to: []
  enables:
    - slug: acting-despite-irreducible-uncertainty
      weight: 0.85
    - slug: accountability-partnership
      weight: 0.78
  requires: []
  alternatives: []
  complementary:
    - slug: adventure-design-methodology
      weight: 0.8
    - slug: adversarial-growth
      weight: 0.78
    - slug: adaptive-leadership-under-uncertainty
      weight: 0.8
  tools: []
# ═══════════════════════════════════════════════════════════════════
# GROUP 6: GRAPH GARDEN (Machine-Written Graph)
# ═══════════════════════════════════════════════════════════════════
graph_garden:
  last_pruned: 2026-02-19
  entities:
    - id: gary-klein
      type: person
      label: "Gary Klein"
      relevance: 0.9
    - id: prospective-hindsight
      type: concept
      label: "Prospective Hindsight"
      relevance: 0.95
    - id: failure-analysis
      type: practice
      label: "Failure Analysis"
      relevance: 0.9
    - id: risk-identification
      type: practice
      label: "Risk Identification"
      relevance: 0.85
    - id: decision-making-framework
      type: framework
      label: "Decision-Making Framework"
      relevance: 0.8
    - id: scenario-planning
      type: practice
      label: "Scenario Planning"
      relevance: 0.85
    - id: psychological-safety
      type: concept
      label: "Psychological Safety"
      relevance: 0.75
  communities:
    - id: decision-making-and-strategy
      label: "Decision-Making and Strategy"
      source: taxonomy
      confidence: 0.95
    - id: risk-management
      label: "Risk Management"
      source: taxonomy
      confidence: 0.9
    - id: futures-thinking
      label: "Futures Thinking"
      source: inferred
      confidence: 0.8
    - id: cognitive-tools-and-practices
      label: "Cognitive Tools and Practices"
      source: inferred
      confidence: 0.85
  inferred_links:
    - target: acting-despite-irreducible-uncertainty
      type: complementary
      confidence: 0.85
      reason: "Pre-mortem identifies risks; acting-despite complements by enabling decisiveness."
    - target: adaptive-strategy-under-uncertainty
      type: complementary
      confidence: 0.82
      reason: "Both address major decisions under uncertainty with forward-looking analysis."
    - target: adventure-design-methodology
      type: complementary
      confidence: 0.78
      reason: "Both frameworks require risk assessment before commitment to major undertakings."
    - target: accountability-partnership
      type: complementary
      confidence: 0.75
      reason: "Partners can support pre-mortem analysis and monitor identified risk mitigation."
    - target: adversarial-growth
      type: complementary
      confidence: 0.72
      reason: "Pre-mortem identifies obstacles; adversarial-growth reframes difficulty constructively."
    - target: active-imagination-technique
      type: tools
      confidence: 0.7
      reason: "Imaginative projection can enhance pre-mortem scenario visualization and depth."
# ═══════════════════════════════════════════════════════════════════
# GROUP 7: PROVENANCE
# ═══════════════════════════════════════════════════════════════════
contributors: ["higgerix", "cloudsters"]
sources:
  - "Gary Klein"
license: CC-BY-SA-4.0
attribution: "commons.engineering by cloudsters, https://cloudsters.net"
---

Before committing to a major life decision, imagine it has already failed and work backward to identify likely causes and preventable risks.

> [!NOTE] Confidence Rating: ★★★ (Established)
> This pattern draws on Gary Klein.

---

### Section 1: Context

We live in an era of accelerating commitments. A startup pivots resources into a new market. A policy team launches a five-year initiative. An activist coalition decides to occupy a site. A product team ships a feature that touches millions of users. Each decision carries real cost: time, capital, relationships, legitimacy, energy.

Yet the moment of decision is typically the *least* informed moment in a system's lifecycle. We're flush with hope, momentum, and incomplete information. We haven't yet lived through the friction, the hidden dependencies, the cascading failures that emerge only in contact with reality.

The Pre-Mortem Analysis pattern arises precisely in this gap—between commitment and consequence. It's needed most acutely in domains where decisions are hard to reverse and failures are publicly visible: project launches, policy rollouts, campaign pivots, system migrations. The pattern asks a simple but heretical question: *What if we treated failure not as an outlier to avoid, but as the baseline we design against?*

This isn't pessimism. It's a form of temporal realism—a way of borrowing the clarity that only hindsight provides, and using it *before* the commitment hardens into fact. In a commons context, this matters because reversing a failed collective decision costs not just capital, but trust, coordination capacity, and stakeholder ownership.

---

### Section 2: Problem

> **The core conflict is Pre vs. Analysis.**

The tension runs like this: *Pre* pulls toward speed, commitment, forward momentum. We need to move. Energy is high. Windows close. Waiting for perfect information is itself a decision—to cede advantage, to let others act first.

*Analysis* pulls toward slowness, depth, interrogation. We need to understand what we're about to do. Hidden risks lurk in assumptions. Failures teach us things we can't yet see. Rushing into a major commitment without scrutiny breeds hollow outcomes.

When *Pre* wins entirely, we commit to decisions we haven't interrogated. Stakeholders later discover they weren't consulted. Dependencies weren't mapped. Failure modes weren't named. The system springs a leak mid-voyage.

When *Analysis* wins entirely, we stay in deliberation indefinitely. Feasibility studies spawn sub-studies. Sign-off layers multiply. The decision window closes. Momentum dies. Nothing gets stewarded into being.

The specific breaking point in this pattern is the *commitment boundary*—the moment when a decision stops being reversible. Once a policy is law, a campaign is public, a system is migrated, the cost of changing course multiplies exponentially. Analysis *after* that boundary becomes reactive damage control, not preventive design.

The keywords matter here: *before committing* is the whole game. You're not analyzing a decision you've already made. You're analyzing it while the decision still lives in potential—while changing course is still cheap, while stakeholders can still be heard, while the design can still bend.

---

### Section 3: Solution

> **Therefore, convene the committed team (or a representative cross-section) in a structured session, ask them to imagine the decision has already been implemented and has failed catastrophically, and harvest their uncensored diagnosis of what went wrong—then use that intelligence to redesign before launch.**

Here's the mechanism: Pre-Mortem Analysis exploits a cognitive asymmetry in how humans think about failure. When asked *directly* "What could go wrong?" during planning, people tend to censor themselves. They soften predictions. They defer to optimism. They protect the group's momentum. But when asked to *inhabit* a failed future and work backward, something shifts. The inhibition lifts. People speak truths they wouldn't otherwise voice.

Gary Klein's insight—tested across military operations, medical teams, and complex projects—is that this imaginative reframing unlocks the *tacit knowledge* already living in practitioners' nervous systems. A nurse knows where a protocol might break down. A campaign organizer feels where a coalition could splinter. A policy designer senses where implementation will hit friction. But this knowledge stays submerged until the right question surfaces it.

The pattern works as a kind of temporal root system. You're not inventing risks; you're harvesting them from the soil of actual experience. Each person on the team carries seeds of failure—near-misses they've witnessed, patterns they've noticed, assumptions they've questioned. The pre-mortem creates structured permission to plant those seeds in the open.

The shift this creates is profound: instead of risks living as quiet anxieties in individual minds, they become *shared, discussable, designable*. Once named, they're no longer destiny. They become design constraints. A broken supply chain becomes a sourcing contingency. A key person leaving becomes a knowledge-transfer protocol. A stakeholder revolt becomes a communication plan. The system gains resilience not by ignoring failure, but by making it visible early.

This is vitality work: you're renewing the system's capacity to sustain itself through contact with reality. You're not generating new value yet; you're protecting what you're about to create.

---

### Section 4: Implementation

**Step 1: Frame the future as failure.**
Assemble the core team or a representative cross-section (8–15 people works well; larger groups fragment into side conversations). Set aside 90 minutes. Open with this exact framing: "Imagine it's 18 months from now. We implemented this decision fully. It has failed. Not just missed a target—genuinely failed. What went wrong?" Don't soften it. The baldness of the prompt is what unlocks honesty.

**Step 2: Generate diagnoses.**
Go around the room. Each person names one likely cause of failure. No discussion, no pushback, no "but we'll have X by then." Write every diagnosis on a wall where everyone can see it. You'll typically surface 20–40 distinct failure modes in the first round. This is healthy. You're draining the swamp.

**Step 3: Cluster and interrogate.**
Group the diagnoses into themes: execution failures, assumption failures, stakeholder failures, technical failures, timing failures. For each cluster, ask: "Which of these are we already mitigating? Which are we blind to?" This is where real design work begins.

**Step 4: Convert to design actions.**
For each unmitigated failure mode, ask: "What would have to be true for this *not* to happen?" This reversal moves the group from diagnosis to design. A "key team member leaves" failure becomes "we have documented decision-making process and cross-trained three people." A "stakeholders feel unheard" becomes "we run feedback sessions at months 3, 6, 9."

---

**Context-specific applications:**

**Corporate (Project Risk Assessment):** In pre-launch product meetings, use pre-mortem to surface: undocumented dependencies between teams, assumptions about user behavior baked into the roadmap, and capacity gaps nobody wanted to name in planning. One tech firm discovered that their rollout assumed the sales team would have trained 50 new hires—but hiring wasn't approved yet. The pre-mortem surfaced this before they shipped.

**Government (Policy Pre-Mortem):** Before a policy launches, convene implementation staff, affected communities, and enforcement bodies. Ask them to imagine the policy has created perverse incentives, bureaucratic chaos, or stakeholder defection. One city discovered through a pre-mortem that their affordable housing policy would actually incentivize developers to *avoid* mixed-income neighborhoods—the opposite of intent. They rewrote the incentive structure before launch.

**Activist (Campaign Risk Analysis):** Before a major campaign action, run a pre-mortem with core organizers and front-line participants. Surface: What narratives could opposition exploit? Where could the coalition fracture? What assumptions about media coverage are fragile? An activist network pre-mortemed a campaign and discovered they'd assumed local media would cover them—but that media was corporate-owned and hostile. They built a distributed media strategy instead.

**Tech (Pre-Mortem AI Facilitator):** Use an AI system to conduct the pre-mortem session: it can harvest diagnoses asynchronously (people feel less inhibited writing to a chatbot than speaking in group), cluster them rapidly, and surface patterns humans might miss. It can also run sensitivity analysis on assumptions. One team used an AI pre-mortem to test how their system would behave under 10 different failure scenarios simultaneously—work that would have taken weeks manually.

**Step 5: Create a risk register.**
Document each failure mode, its likelihood, its impact if it occurs, and the specific design action you'll take to prevent it. This becomes a living document—reviewed at month 3, month 6, before major gates. It's not a static artifact; it's a stewardship tool.

---

### Section 5: Consequences

**What flourishes:**

Pre-Mortem Analysis shifts the system from *defending against unknown risks* to *designing with known ones*. Unspoken anxieties become explicit design constraints. The group moves from hoping failure won't happen to engineering against failure actually happening. This builds what we might call "confident humility"—high commitment paired with realistic caution.

Stakeholder ownership deepens. When people are invited to name what could go wrong, they feel heard. Their tacit knowledge is valued. When design changes reflect their pre-mortem insights, they shift from skeptics to invested stakeholders. The decision becomes collectively owned in a way planning alone never creates.

The pattern also catalyzes lateral learning. A policy team discovers that a similar initiative failed in another jurisdiction for reasons nobody in their room had anticipated. A product team learns that their competitor hit exactly the failure mode they were worried about. The pre-mortem becomes a listening device for organizational learning.

**What risks emerge:**

The biggest decay risk is *ritualization without teeth*. Teams run pre-mortems, surface real failures, write them down—then return to execution as if the pre-mortem never happened. The diagnoses become theater. This happens when there's no follow-up mechanism, when accountability for mitigating risks isn't assigned, when the register isn't reviewed.

A second risk: *false precision*. Teams can mistake the pre-mortem for prophecy. They identify certain failure modes and build elaborate mitigations—only to discover the *actual* failure came from a direction they didn't anticipate. The pre-mortem isn't fortune-telling; it's pattern recognition. Residual uncertainty remains.

Given that this pattern's resilience score is 3.0, note that pre-mortems don't build adaptive capacity in real time. They prepare you for *known unknowns*, but *unknown unknowns* still blindside you. The pattern sustains existing system health but doesn't necessarily generate new learning capacity or emergence. Teams can become overconfident in their risk management and underprepared for novel failure modes.

---

### Section 6: Known Uses

**Gary Klein's Hospital Case (source tradition):**
Klein introduced pre-mortems in a hospital system preparing to implement a new patient handoff protocol. The existing protocol had caused errors; the new one was supposed to fix them. In the pre-mortem, nurses and physicians imagined the new protocol had launched and *increased* errors. They surfaced: the new forms required information that wasn't available at handoff time, creating a bottleneck; the system assumed all staff had been trained, but night shift hadn't; and the protocol created a new communication gap between certain roles. The hospital redesigned the protocol to address each constraint before launch. The actual rollout encountered none of the imagined failures because they'd been designed out. The protocol worked.

**City Budget Crisis Prevention:**
A mid-size city planning a major budget reallocation ran a pre-mortem with department heads, council members, and community representatives. They imagined the new budget had caused a service collapse. The pre-mortem surfaced: the finance team had assumed departments would absorb 15% cuts without laying off staff (unrealistic); council had promised community benefits that required ongoing funding (one-time budget cuts would break those promises); and the unions hadn't been consulted and would almost certainly grieve layoffs. The city restructured the reallocation as a three-year transition with union negotiation built in, contingency funding for essential services, and community benefit commitments that matched actual revenue. The budget passed without crisis.

**Tech Product Pivot (Anonymous, observed):**
A SaaS company pre-mortemed a product pivot into a new market segment. The leadership team imagined the pivot had failed: they'd overestimated how quickly the new segment would adopt, underestimated how much customization the segment required, and discovered that their core product didn't address the segment's actual pain point—they'd been selling to a proxy buyer, not the end user. The pre-mortem led them to run a six-week pilot with real users *before* full pivot. The pilot revealed exactly those gaps. They redesigned their go-to-market strategy, rewrote the product roadmap, and changed pricing. The eventual pivot succeeded because it was designed against the failures they'd pre-mortemed.

---

### Section 7: Cognitive Era

In an age of distributed intelligence and AI-mediated decision-making, Pre-Mortem Analysis becomes both more powerful and more fragile.

More powerful: An AI Pre-Mortem Facilitator can run distributed sessions asynchronously, harvesting failure diagnoses from teams across geographies and time zones. It can surface patterns across dozens of similar decisions made elsewhere—showing you exactly how comparable initiatives have failed. It can run rapid sensitivity analysis: "If this assumption is wrong by 30%, what fails first?" It can generate failure scenarios you didn't think to imagine.

Yet here's the risk: the pattern's core mechanism is *human vulnerability speaking truth*. An AI can aggregate data about failure, but can it create the psychological safety where a junior organizer speaks the doubt that senior leadership is trying to suppress? Can it harvest the embodied knowledge—the gut feeling, the pattern recognized through years of lived experience—that doesn't yet have language? These are frontier questions.

The second shift is *speed of consequence*. In classical Gary Klein contexts (surgery, military ops), failure took hours to manifest. Teams had time to learn. In modern tech systems, failures can cascade globally in seconds. A pre-mortem designed to prevent failures that emerge in 18 months might be blindsided by a failure that propagates in 18 minutes. The pattern assumes time for redesign; AI-mediated systems may not grant that luxury.

The third shift is *stakeholder access*. In a commons context, whose voices does the AI pre-mortem include? Does it surface failure modes that marginalized stakeholders have anticipated but leadership hasn't heard? Or does it amplify the blind spots of whoever designed the AI? A pre-mortem that includes frontline workers, affected communities, and dissenting voices is more robust than one that only interrogates a leadership team.

---

### Section 8: Vitality

**Signs of life:**

The pattern is thriving when: (1) Team members surface *surprising* failure diagnoses—ideas nobody had voiced before, especially from junior or marginalized voices. (2) Design changes made between the pre-mortem and launch reflect the diagnoses—you can trace a specific protocol change, a stakeholder consultation, a contingency plan back to "we pre-mortemed this risk." (3) When the system does encounter difficulties, people reference the pre-mortem: "This is exactly what we imagined," showing that the pre-mortem became part of collective memory, not forgotten ceremony. (4) The pre-mortem register is *actively maintained*—reviewed at decision gates, updated as circumstances change, used to adjust course mid-implementation.

**Signs of decay:**

The pattern is hollow when: (1) Pre-mortems are run but the register is never reviewed. Failure modes are named, then ignored. (2) The same failures surface across successive pre-mortems (for different initiatives), indicating nothing was learned or mitigated. (3) Leadership dismisses diagnoses as pessimistic or obstructive; people stop offering real concerns. (4) The pre-mortem becomes a compliance box—"we did the pre-mortem"—with no actual design change downstream. When you see these signals, the pattern has calcified into ritual.

**When to replant:**

Restart the pre-mortem practice when you're entering a new phase of complexity (scaling to a new market, merging teams, shifting core assumptions). Consider redesigning it when: the organization has grown enough that psychological safety has fragmented and people are no longer willing to name concerns publicly—you may need a facilitator, anonymous input channels, or smaller group sessions. Or when you've hit a failure that the pre-mortem *didn't* catch—that's data telling you your model of risk needs renewal. That's the moment to ask: *What were we blind to? What would need to be true for us to see it next time?*
